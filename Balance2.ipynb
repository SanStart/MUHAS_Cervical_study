{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from skimage.util import random_noise\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.restoration import denoise_tv_chambolle, denoise_bilateral\n",
    "import random\n",
    "\n",
    "def divideAndBalanceByClass(n_classes_balance, test_size):\n",
    "    read_folder = \"D:\\\\Old_PC\\\\doc\\\\Projects\\\\Elective_Study\\\\Dataset\\\\New_CRIC_Dataset_6\"\n",
    "    classes = os.listdir(read_folder)\n",
    "    X_train_bal, y_train_bal, X_validation_bal, y_validation_bal, X_test_bal, y_test_bal = [], [], [], [], [], []\n",
    "    \n",
    "    x = 0 #ASCH\n",
    "    read_folder2 = read_folder + '\\\\' + classes[x] + '\\\\'\n",
    "    image_names = os.listdir(read_folder2)\n",
    "    imgs_train, imgs_test = train_test_split(image_names, test_size=test_size)\n",
    "    X_train, y_train, X_test, y_test = dataAugmentationASCH(imgs_train, imgs_test, n_classes_balance, read_folder2)\n",
    "    X_train_part, X_validation, y_train_part, y_validation = train_test_split(X_train, y_train, test_size=test_size)\n",
    "    X_train_bal = X_train_bal + X_train_part\n",
    "    y_train_bal = y_train_bal + y_train_part\n",
    "    X_validation_bal = X_validation_bal + X_validation\n",
    "    y_validation_bal = y_validation_bal + y_validation\n",
    "    X_test_bal = X_test_bal + X_test\n",
    "    y_test_bal = y_test_bal + y_test\n",
    "    \n",
    "    x = 1 #ASCUS\n",
    "    read_folder2 = read_folder + '\\\\' + classes[x] + '\\\\'\n",
    "    image_names = os.listdir(read_folder2)\n",
    "    imgs_train, imgs_test = train_test_split(image_names, test_size=test_size)\n",
    "    X_train, y_train, X_test, y_test = dataAugmentationASCUS(imgs_train, imgs_test, n_classes_balance, read_folder2)\n",
    "    X_train_part, X_validation, y_train_part, y_validation = train_test_split(X_train, y_train, test_size=test_size)\n",
    "    X_train_bal = X_train_bal + X_train_part\n",
    "    y_train_bal = y_train_bal + y_train_part\n",
    "    X_validation_bal = X_validation_bal + X_validation\n",
    "    y_validation_bal = y_validation_bal + y_validation\n",
    "    X_test_bal = X_test_bal + X_test\n",
    "    y_test_bal = y_test_bal + y_test\n",
    "    \n",
    "    x = 2 #CA\n",
    "    read_folder2 = read_folder + '\\\\' + classes[x] + '\\\\'\n",
    "    image_names = os.listdir(read_folder2)\n",
    "    imgs_train, imgs_test = train_test_split(image_names, test_size=test_size)\n",
    "    X_train, y_train, X_test, y_test = dataAugmentationCA(imgs_train, imgs_test, n_classes_balance, read_folder2)\n",
    "    X_train_part, X_validation, y_train_part, y_validation = train_test_split(X_train, y_train, test_size=test_size)\n",
    "    X_train_bal = X_train_bal + X_train_part\n",
    "    y_train_bal = y_train_bal + y_train_part\n",
    "    X_validation_bal = X_validation_bal + X_validation\n",
    "    y_validation_bal = y_validation_bal + y_validation\n",
    "    X_test_bal = X_test_bal + X_test\n",
    "    y_test_bal = y_test_bal + y_test\n",
    "    \n",
    "    x = 3 #HSIL\n",
    "    read_folder2 = read_folder + '\\\\' + classes[x] + '\\\\'\n",
    "    image_names = os.listdir(read_folder2)\n",
    "    imgs_train, imgs_test = train_test_split(image_names, test_size=test_size)\n",
    "    X_train, y_train, X_test, y_test = dataAugmentationHSIL(imgs_train, imgs_test, n_classes_balance, read_folder2)\n",
    "    X_train_part, X_validation, y_train_part, y_validation = train_test_split(X_train, y_train, test_size=test_size)\n",
    "    X_train_bal = X_train_bal + X_train_part\n",
    "    y_train_bal = y_train_bal + y_train_part\n",
    "    X_validation_bal = X_validation_bal + X_validation\n",
    "    y_validation_bal = y_validation_bal + y_validation\n",
    "    X_test_bal = X_test_bal + X_test\n",
    "    y_test_bal = y_test_bal + y_test\n",
    "    \n",
    "    x = 4 #LSIL\n",
    "    read_folder2 = read_folder + '\\\\' + classes[x] + '\\\\'\n",
    "    image_names = os.listdir(read_folder2)\n",
    "    imgs_train, imgs_test = train_test_split(image_names, test_size=test_size)\n",
    "    X_train, y_train, X_test, y_test = dataAugmentationLSIL(imgs_train, imgs_test, n_classes_balance, read_folder2)\n",
    "    X_train_part, X_validation, y_train_part, y_validation = train_test_split(X_train, y_train, test_size=test_size)\n",
    "    X_train_bal = X_train_bal + X_train_part\n",
    "    y_train_bal = y_train_bal + y_train_part\n",
    "    X_validation_bal = X_validation_bal + X_validation\n",
    "    y_validation_bal = y_validation_bal + y_validation\n",
    "    X_test_bal = X_test_bal + X_test\n",
    "    y_test_bal = y_test_bal + y_test\n",
    "    \n",
    "    x = 5 #Normal\n",
    "    read_folder2 = read_folder + '\\\\' + classes[x] + '\\\\'\n",
    "    image_names = os.listdir(read_folder2)\n",
    "    imgs_train, imgs_test = train_test_split(image_names, test_size=test_size)\n",
    "    X_train, y_train, X_test, y_test = dataAugmentationNormal(imgs_train, imgs_test, n_classes_balance, read_folder2)\n",
    "    X_train_part, X_validation, y_train_part, y_validation = train_test_split(X_train, y_train, test_size=test_size)\n",
    "    X_train_bal = X_train_bal + X_train_part\n",
    "    y_train_bal = y_train_bal + y_train_part\n",
    "    X_validation_bal = X_validation_bal + X_validation\n",
    "    y_validation_bal = y_validation_bal + y_validation\n",
    "    X_test_bal = X_test_bal + X_test\n",
    "    y_test_bal = y_test_bal + y_test\n",
    "    \n",
    "    return X_train_bal, y_train_bal, X_validation_bal, y_validation_bal, X_test_bal, y_test_bal\n",
    "\n",
    "\n",
    "\n",
    "def dataAugmentationLSIL(training_images, test_images, n_classes, class_folder):\n",
    "    # For 6 classes -> 220 images become +1\n",
    "    # For 3 classes -> each image becomes +1, 100 images become +1\n",
    "    # For 2 classes -> 220 images become +1\n",
    "    n_images = len(training_images)\n",
    "    X_training = []\n",
    "    y_training = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    if(n_classes == 6):\n",
    "        class_id = 4 #LSIL\n",
    "    elif(n_classes == 2):\n",
    "        class_id = 0\n",
    "    else:\n",
    "        class_id = 1\n",
    "    for x in range(0,n_images):\n",
    "        img = cv2.imread(class_folder + training_images[x])\n",
    "        X_training.append(img)\n",
    "        y_training.append(class_id)\n",
    "    if (n_classes == 3):\n",
    "        image_operations = random.sample(range(0,n_images), n_images)\n",
    "        for pos in range(0,len(image_operations)):\n",
    "            if(pos < 100):\n",
    "                operations = random.sample(range(1,10), 2)\n",
    "            else:\n",
    "                operations = random.sample(range(1,10), 1)\n",
    "            for operation in operations:\n",
    "                new_img = performAugmentationOperation(operation, X_training[pos])\n",
    "                X_training.append(new_img)\n",
    "                y_training.append(class_id)\n",
    "    if (n_classes == 6 or n_classes == 2):\n",
    "        image_operations = random.sample(range(0,n_images), 220)\n",
    "        for img_op in image_operations:\n",
    "            operation = random.randint(1,10)\n",
    "            new_img = performAugmentationOperation(operation, X_training[img_op])\n",
    "            X_training.append(new_img)\n",
    "            y_training.append(class_id)\n",
    "    for x in range(0,len(test_images)):\n",
    "        X_test.append(cv2.imread(class_folder + test_images[x]))\n",
    "        y_test.append(class_id)\n",
    "    return X_training, y_training, X_test, y_test\n",
    "\n",
    "\n",
    "def dataAugmentationLSIL(training_images, test_images, n_classes, class_folder):\n",
    "    # For 6 classes -> 220 images become +1\n",
    "    # For 3 classes -> each image becomes +1, 100 images become +1\n",
    "    # For 2 classes -> 220 images become +1\n",
    "    n_images = len(training_images)\n",
    "    X_training = []\n",
    "    y_training = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    if(n_classes == 6):\n",
    "        class_id = 4 #LSIL\n",
    "    elif(n_classes == 2):\n",
    "        class_id = 0\n",
    "    else:\n",
    "        class_id = 1\n",
    "    for x in range(0,n_images):\n",
    "        img = cv2.imread(class_folder + training_images[x])\n",
    "        X_training.append(img)\n",
    "        y_training.append(class_id)\n",
    "    if (n_classes == 3):\n",
    "        image_operations = random.sample(range(0,n_images), n_images)\n",
    "        for pos in range(0,len(image_operations)):\n",
    "            if(pos < 100):\n",
    "                operations = random.sample(range(1,10), 2)\n",
    "            else:\n",
    "                operations = random.sample(range(1,10), 1)\n",
    "            for operation in operations:\n",
    "                new_img = performAugmentationOperation(operation, X_training[pos])\n",
    "                X_training.append(new_img)\n",
    "                y_training.append(class_id)\n",
    "    if (n_classes == 6 or n_classes == 2):\n",
    "        image_operations = random.sample(range(0,n_images), 220)\n",
    "        for img_op in image_operations:\n",
    "            operation = random.randint(1,10)\n",
    "            new_img = performAugmentationOperation(operation, X_training[img_op])\n",
    "            X_training.append(new_img)\n",
    "            y_training.append(class_id)\n",
    "    for x in range(0,len(test_images)):\n",
    "        X_test.append(cv2.imread(class_folder + test_images[x]))\n",
    "        y_test.append(class_id)\n",
    "    return X_training, y_training, X_test, y_test\n",
    "\n",
    "def dataAugmentationASCUS(training_images, test_images, n_classes, class_folder):\n",
    "    # For 6 classes -> each image becomes +1\n",
    "    # For 3 classes -> each image becomes +3, 130 images become +1\n",
    "    # For 2 classes -> each image becomes +1\n",
    "    n_images = len(training_images)\n",
    "    X_training = []\n",
    "    y_training = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    if(n_classes == 6):\n",
    "        class_id = 1 #ASCUS\n",
    "    elif(n_classes == 2):\n",
    "        class_id = 0\n",
    "    else:\n",
    "        class_id = 1\n",
    "    for x in range(0, n_images):\n",
    "        img = cv2.imread(class_folder + training_images[x])\n",
    "        X_training.append(img)\n",
    "        y_training.append(class_id)\n",
    "    if (n_classes == 3):\n",
    "        image_operations = random.sample(range(0, n_images), n_images)\n",
    "        for pos in range(0, len(image_operations)):\n",
    "            if(pos < 130):\n",
    "                operations = random.sample(range(1, 10), 4)\n",
    "            else:\n",
    "                operations = random.sample(range(1, 10), 3)\n",
    "            for operation in operations:\n",
    "                new_img = performAugmentationOperation(operation, X_training[pos])\n",
    "                X_training.append(new_img)\n",
    "                y_training.append(class_id)\n",
    "    if (n_classes == 6 or n_classes == 2):\n",
    "        for x in range(0, n_images):\n",
    "            operations = random.sample(range(1, 10), 2)\n",
    "            for operation in operations:\n",
    "                new_img = performAugmentationOperation(operation, X_training[x])\n",
    "                X_training.append(new_img)\n",
    "                y_training.append(class_id)\n",
    "    for x in range(0, len(test_images)):\n",
    "        X_test.append(cv2.imread(class_folder + test_images[x]))\n",
    "        y_test.append(class_id)\n",
    "    return X_training, y_training, X_test, y_test\n",
    "\n",
    "def dataAugmentationHSIL(training_images, test_images, n_classes, class_folder):\n",
    "    # For 6 classes -> no balancing\n",
    "    # For 3 classes -> no balancing\n",
    "    # For 2 classes -> no balancing\n",
    "    n_images = len(training_images)\n",
    "    X_training = []\n",
    "    y_training = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    if(n_classes == 6):\n",
    "        class_id = 3 #HSIL\n",
    "    else:\n",
    "        class_id = 0\n",
    "    for x in range(0, n_images):\n",
    "        img = cv2.imread(class_folder + training_images[x])\n",
    "        X_training.append(img)\n",
    "        y_training.append(class_id)\n",
    "    for x in range(0, len(test_images)):\n",
    "        X_test.append(cv2.imread(class_folder + test_images[x]))\n",
    "        y_test.append(class_id)\n",
    "    return X_training, y_training, X_test, y_test\n",
    "\n",
    "def dataAugmentationASCH(training_images, test_images, n_classes, class_folder):\n",
    "    # For 6 classes -> 270 images become +1\n",
    "    # For 3 classes -> 270 images become +1\n",
    "    # For 2 classes -> 270 images become +1\n",
    "    n_images = len(training_images)\n",
    "    X_training = []\n",
    "    y_training = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    class_id = 0 #ASCH\n",
    "    for x in range(0, n_images):\n",
    "        img = cv2.imread(class_folder + training_images[x])\n",
    "        X_training.append(img)\n",
    "        y_training.append(class_id)\n",
    "    image_operations = random.sample(range(0, n_images), 270)\n",
    "    for img_op in image_operations:\n",
    "        operation = random.randint(1, 10)\n",
    "        new_img = performAugmentationOperation(operation, X_training[img_op])\n",
    "        X_training.append(new_img)\n",
    "        y_training.append(class_id)\n",
    "    for x in range(0, len(test_images)):\n",
    "        X_test.append(cv2.imread(class_folder + test_images[x]))\n",
    "        y_test.append(class_id)\n",
    "    return X_training, y_training, X_test, y_test\n",
    "\n",
    "def dataAugmentationCA(training_images, test_images, n_classes, class_folder):\n",
    "    # For 6 classes -> each CA image becomes +10\n",
    "    # For 3 classes -> each CA image becomes +10\n",
    "    # For 2 classes -> each CA image becomes +10\n",
    "    n_images = len(training_images)\n",
    "    X_training = []\n",
    "    y_training = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    if(n_classes == 6):\n",
    "        class_id = 2 #CA\n",
    "    else:\n",
    "        class_id = 0\n",
    "    for x in range(0, n_images):\n",
    "        img = cv2.imread(class_folder + training_images[x])\n",
    "        X_training.append(img)\n",
    "        y_training.append(class_id)\n",
    "        for operation in range(1, 11):\n",
    "            new_img = performAugmentationOperation(operation, img)\n",
    "            X_training.append(new_img)\n",
    "            y_training.append(class_id)\n",
    "    for x in range(0, len(test_images)):\n",
    "        X_test.append(cv2.imread(class_folder + test_images[x]))\n",
    "        y_test.append(class_id)\n",
    "    return X_training, y_training, X_test, y_test\n",
    "    \n",
    "\n",
    "def dataAugmentationNormal(training_images, test_images, n_classes, class_folder):\n",
    "    # For 6 classes -> already balanced\n",
    "    # For 3 classes -> each normal image becomes +2\n",
    "    # For 2 classes -> each image becomes +4\n",
    "    n_images = len(training_images)\n",
    "    X_training = []\n",
    "    y_training = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    if(n_classes == 6):\n",
    "        class_id = 5 #Normal\n",
    "    elif(n_classes == 3):\n",
    "        class_id = 2\n",
    "    else:\n",
    "        class_id = 1\n",
    "    if(n_classes == 6):\n",
    "    # No balancing needed\n",
    "        for x in range(0, n_images):\n",
    "            img = cv2.imread(class_folder + training_images[x])\n",
    "            X_training.append(img)\n",
    "            y_training.append(class_id)\n",
    "    if(n_classes == 3):\n",
    "    # Each image becomes +2\n",
    "        for x in range(0, n_images):\n",
    "            img = cv2.imread(class_folder + training_images[x])\n",
    "            X_training.append(img)\n",
    "            y_training.append(class_id)\n",
    "            operations = random.sample(range(1, 10), 4)\n",
    "            for operation in operations:\n",
    "                new_img = performAugmentationOperation(operation, img)\n",
    "                X_training.append(new_img)\n",
    "                y_training.append(class_id)\n",
    "    if(n_classes == 2):\n",
    "    # Each image becomes +4\n",
    "        for x in range(0, n_images):\n",
    "            img = cv2.imread(class_folder + training_images[x])\n",
    "            X_training.append(img)\n",
    "            y_training.append(class_id)\n",
    "            operations = random.sample(range(1, 10), 4)\n",
    "            for operation in operations:\n",
    "                new_img = performAugmentationOperation(operation, img)\n",
    "                X_training.append(new_img)\n",
    "                y_training.append(class_id)\n",
    "    for x in range(0, len(test_images)):\n",
    "        X_test.append(cv2.imread(class_folder + test_images[x]))\n",
    "        y_test.append(class_id)\n",
    "    return X_training, y_training, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def performAugmentationOperation(operation, img):\n",
    "    # rotate\n",
    "    if(operation == 1):\n",
    "        new_img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    elif(operation == 2):\n",
    "        new_img = cv2.rotate(img, cv2.ROTATE_180)\n",
    "    elif(operation == 3):\n",
    "        new_img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    # flip\n",
    "    elif(operation == 4):\n",
    "        new_img = cv2.flip(img, 1)\n",
    "    elif(operation == 5):\n",
    "        img_rotate_90 = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "        new_img = cv2.flip(img_rotate_90, 1)\n",
    "    elif(operation == 6):\n",
    "        img_rotate_180 = cv2.rotate(img, cv2.ROTATE_180)\n",
    "        new_img = cv2.flip(img_rotate_180, 1)\n",
    "    elif(operation == 7):\n",
    "        img_rotate_270 = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        new_img = cv2.flip(img_rotate_270, 1)\n",
    "    elif(operation == 8):\n",
    "        sigma = 0.05 \n",
    "        noisy = random_noise(img, var=sigma**2)\n",
    "        new_img = noisy\n",
    "        new_img = new_img * 255\n",
    "    elif(operation == 9):\n",
    "        sigma = 0.005 \n",
    "        noisy = random_noise(img, var=sigma**2)\n",
    "        new_img = denoise_tv_chambolle(noisy, weight=0.1, channel_axis=-1)\n",
    "\n",
    "        new_img = new_img * 255\n",
    "    elif(operation == 10):\n",
    "        sigma = 0.005 \n",
    "        noisy = random_noise(img, var=sigma**2)\n",
    "        new_img = denoise_bilateral(noisy, sigma_color=0.05, sigma_spatial=15, channel_axis=-1)\n",
    "\n",
    "        new_img = new_img * 255\n",
    "    return new_img\n",
    "\n",
    "\n",
    "def save_DividedBalanced(num_classes):\n",
    "    X_train, y_train, X_validation, y_validation, X_test, y_test = divideAndBalanceByClass(num_classes, 0.2)\n",
    "    save_folder = \"D:\\\\Old_PC\\\\doc\\\\Projects\\\\Elective_Study\\\\Dataset\\\\6classes\"\n",
    "    for x in range(0, len(X_train)):\n",
    "        new_name = save_folder + \"\\\\Train\\\\\" + str(x) + \"_\" + str(y_train[x]) + \"_\" + \".png\"\n",
    "        cv2.imwrite(new_name, X_train[x])\n",
    "    for x in range(0, len(X_validation)):\n",
    "        new_name = save_folder + \"\\\\Validation\\\\\" + str(x) + \"_\" + str(y_validation[x]) + \"_\" + \".png\"\n",
    "        cv2.imwrite(new_name, X_validation[x])\n",
    "    for x in range(0, len(X_test)):\n",
    "        new_name = save_folder + \"\\\\Test\\\\\" + str(x) + \"_\" + str(y_test[x]) + \"_\" + \".png\"\n",
    "        cv2.imwrite(new_name, X_test[x])\n",
    "        \n",
    "def read_DividedBalanced(num_classes):\n",
    "    read_folder = \"D:\\\\Old_PC\\\\doc\\\\Projects\\\\Elective_Study\\\\Dataset\\\\6classes\"\n",
    "    X_train, y_train, X_validation, y_validation, X_test, y_test = [], [], [], [], [], []\n",
    "    for file in os.listdir(read_folder + '\\\\Train\\\\'):\n",
    "        X_train.append(cv2.imread(read_folder + '\\\\Train\\\\' + file))\n",
    "        y_train.append(file.split('_')[1])\n",
    "    for file in os.listdir(read_folder + '\\\\Validation\\\\'):\n",
    "        X_validation.append(cv2.imread(read_folder + '\\\\Validation\\\\' + file))\n",
    "        y_validation.append(file.split('_')[1])\n",
    "    for file in os.listdir(read_folder + '\\\\Test\\\\'):\n",
    "        X_test.append(cv2.imread(read_folder + '\\\\Test\\\\' + file))\n",
    "        y_test.append(file.split('_')[1])\n",
    "    return X_train, y_train, X_validation, y_validation, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"D:\\\\Old_PC\\\\doc\\\\Projects\\\\Elective_Study\\\\Dataset\\\\6classes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = save_folder + '\\\\Train\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Old_PC\\doc\\Projects\\Elective_Study\\Dataset\\6classes\\Train\\\n"
     ]
    }
   ],
   "source": [
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import ResNet152, ResNet50, MobileNet, InceptionV3\n",
    "import efficientnet.tfkeras as efn\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import multilabel_confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from Balance import read_DividedBalanced\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Helper: Wilson score interval\n",
    "# ----------------------------\n",
    "def wilson_ci(k, n, alpha=0.05):\n",
    "    \"\"\"\n",
    "    k: number correct\n",
    "    n: total\n",
    "    returns (lower, upper)\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "        return (0.0, 0.0)\n",
    "    from math import sqrt\n",
    "    z = 1.96  # for 95% CI\n",
    "    phat = k / n\n",
    "    denom = 1 + z**2 / n\n",
    "    centre = phat + z**2 / (2 * n)\n",
    "    margin = z * math.sqrt((phat * (1 - phat) + z**2 / (4 * n)) / n)\n",
    "    lower = (centre - margin) / denom\n",
    "    upper = (centre + margin) / denom\n",
    "    return lower, upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Build classifier function\n",
    "# ----------------------------\n",
    "def buildClassifier(num_classes, classifier_num, input_shape=(90, 90, 3), l2_lambda=0.001, dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    classifier_num mapping:\n",
    "    1 -> ResNet152\n",
    "    2 -> ResNet50\n",
    "    3 -> EfficientNetB7\n",
    "    4 -> MobileNet\n",
    "    5 -> InceptionV3\n",
    "    All models use pretrained ImageNet weights and are fine-tuned.\n",
    "    \"\"\"\n",
    "    weights = 'imagenet'  # use transfer learning\n",
    "    if classifier_num == 1:\n",
    "        base_model = ResNet152(include_top=False, input_shape=input_shape, weights=weights)\n",
    "    elif classifier_num == 2:\n",
    "        base_model = ResNet50(include_top=False, input_shape=input_shape, weights=weights)\n",
    "    elif classifier_num == 3:\n",
    "        base_model = efn.EfficientNetB7(include_top=False, input_shape=input_shape, weights=weights)\n",
    "    elif classifier_num == 4:\n",
    "        base_model = MobileNet(include_top=False, input_shape=input_shape, weights=weights)\n",
    "    elif classifier_num == 5:\n",
    "        base_model = InceptionV3(include_top=False, input_shape=input_shape, weights=weights)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid classifier number\")\n",
    "\n",
    "    # Optionally allow fine-tuning: unfreeze last blocks if desired\n",
    "    # For now we fine-tune entire model (you can freeze earlier layers if needed)\n",
    "    base_model.trainable = True\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(dropout_rate)(x)  # dropout in fully connected head\n",
    "    predictions = Dense(num_classes, activation='softmax',\n",
    "                        kernel_regularizer=l2(l2_lambda))(x)  # L2 regularization\n",
    "    classifier = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Use Adam with requested parameters and categorical crossentropy\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999)\n",
    "    classifier.compile(optimizer=optimizer,\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=[Precision(name='precision'), Recall(name='recall'), 'accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Print aggregated metrics\n",
    "# ----------------------------\n",
    "def printResults(mcm):\n",
    "    # mcm is multilabel_confusion_matrix with shape (num_classes, 2, 2)\n",
    "    tn = np.mean(mcm[:,0,0])\n",
    "    tp = np.mean(mcm[:,1,1])\n",
    "    fn = np.mean(mcm[:,1,0])\n",
    "    fp = np.mean(mcm[:,0,1])\n",
    "    precision = round(tp / (tp + fp + 1e-12), 2)\n",
    "    recall = round(tp / (tp + fn + 1e-12), 2)\n",
    "    f1 = round((2 * precision * recall) / (precision + recall + 1e-12), 2)\n",
    "    accuracy = round((tp + tn) / (tp + tn + fp + fn + 1e-12), 2)\n",
    "    Specificity = round(tn / (tn + fp + 1e-12), 2)\n",
    "    Sensitivity = round(tp / (tp + fn + 1e-12), 2)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Specificity:\", Specificity)\n",
    "    print(\"Sensitivity:\", Sensitivity)\n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': accuracy,\n",
    "            'specificity': Specificity, 'sensitivity': Sensitivity}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Load data (user-provided function)\n",
    "# ----------------------------\n",
    "num_classes = 6\n",
    "# Adjust these shapes if your read function returns already-resized images\n",
    "X_train, y_train, X_validation, y_validation, X_test, y_test = read_DividedBalanced(num_classes)\n",
    "\n",
    "# Convert and normalize to float32 [0,1]\n",
    "X_train = np.array(X_train, dtype=np.float32) / 255.0\n",
    "X_validation = np.array(X_validation, dtype=np.float32) / 255.0\n",
    "X_test = np.array(X_test, dtype=np.float32) / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_validation = to_categorical(y_validation, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Data augmentation\n",
    "# ----------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.7896 - precision: 0.8211 - recall: 0.7047 - accuracy: 0.7534\n",
      "Epoch 1: val_recall improved from -inf to 0.54909, saving model to best_model_classifier_1_6classes.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ENVY 14\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/275 [==============================] - 2602s 9s/step - loss: 0.7896 - precision: 0.8211 - recall: 0.7047 - accuracy: 0.7534 - val_loss: 194.8240 - val_precision: 0.5491 - val_recall: 0.5491 - val_accuracy: 0.5491 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.3884 - precision: 0.8909 - recall: 0.8289 - accuracy: 0.8612 \n",
      "Epoch 2: val_recall did not improve from 0.54909\n",
      "275/275 [==============================] - 3218s 12s/step - loss: 0.3884 - precision: 0.8909 - recall: 0.8289 - accuracy: 0.8612 - val_loss: 14.6219 - val_precision: 0.0514 - val_recall: 0.0159 - val_accuracy: 0.0532 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.3391 - precision: 0.9075 - recall: 0.8606 - accuracy: 0.8829 \n",
      "Epoch 3: val_recall did not improve from 0.54909\n",
      "275/275 [==============================] - 4046s 15s/step - loss: 0.3391 - precision: 0.9075 - recall: 0.8606 - accuracy: 0.8829 - val_loss: 2.4206 - val_precision: 0.3783 - val_recall: 0.2868 - val_accuracy: 0.3250 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.2801 - precision: 0.9208 - recall: 0.8829 - accuracy: 0.9028\n",
      "Epoch 4: val_recall improved from 0.54909 to 0.80909, saving model to best_model_classifier_1_6classes.hdf5\n",
      "275/275 [==============================] - 2413s 9s/step - loss: 0.2801 - precision: 0.9208 - recall: 0.8829 - accuracy: 0.9028 - val_loss: 0.9052 - val_precision: 0.8476 - val_recall: 0.8091 - val_accuracy: 0.8259 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.2522 - precision: 0.9317 - recall: 0.9041 - accuracy: 0.9186\n",
      "Epoch 5: val_recall improved from 0.80909 to 0.87091, saving model to best_model_classifier_1_6classes.hdf5\n",
      "275/275 [==============================] - 2308s 8s/step - loss: 0.2522 - precision: 0.9317 - recall: 0.9041 - accuracy: 0.9186 - val_loss: 28.7308 - val_precision: 0.8899 - val_recall: 0.8709 - val_accuracy: 0.8805 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.2282 - precision: 0.9340 - recall: 0.9133 - accuracy: 0.9237\n",
      "Epoch 6: val_recall improved from 0.87091 to 0.91545, saving model to best_model_classifier_1_6classes.hdf5\n",
      "275/275 [==============================] - 2248s 8s/step - loss: 0.2282 - precision: 0.9340 - recall: 0.9133 - accuracy: 0.9237 - val_loss: 8.9343 - val_precision: 0.9273 - val_recall: 0.9155 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1869 - precision: 0.9458 - recall: 0.9288 - accuracy: 0.9368\n",
      "Epoch 7: val_recall improved from 0.91545 to 0.92182, saving model to best_model_classifier_1_6classes.hdf5\n",
      "275/275 [==============================] - 2174s 8s/step - loss: 0.1869 - precision: 0.9458 - recall: 0.9288 - accuracy: 0.9368 - val_loss: 0.9737 - val_precision: 0.9298 - val_recall: 0.9218 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1579 - precision: 0.9544 - recall: 0.9406 - accuracy: 0.9476\n",
      "Epoch 8: val_recall improved from 0.92182 to 0.93364, saving model to best_model_classifier_1_6classes.hdf5\n",
      "275/275 [==============================] - 2507s 9s/step - loss: 0.1579 - precision: 0.9544 - recall: 0.9406 - accuracy: 0.9476 - val_loss: 0.2134 - val_precision: 0.9444 - val_recall: 0.9336 - val_accuracy: 0.9373 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1535 - precision: 0.9560 - recall: 0.9437 - accuracy: 0.9494\n",
      "Epoch 9: val_recall did not improve from 0.93364\n",
      "275/275 [==============================] - 1953s 7s/step - loss: 0.1535 - precision: 0.9560 - recall: 0.9437 - accuracy: 0.9494 - val_loss: 0.2479 - val_precision: 0.9461 - val_recall: 0.9332 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1417 - precision: 0.9613 - recall: 0.9506 - accuracy: 0.9564\n",
      "Epoch 10: val_recall improved from 0.93364 to 0.93955, saving model to best_model_classifier_1_6classes.hdf5\n",
      "275/275 [==============================] - 1898s 7s/step - loss: 0.1417 - precision: 0.9613 - recall: 0.9506 - accuracy: 0.9564 - val_loss: 0.5706 - val_precision: 0.9447 - val_recall: 0.9395 - val_accuracy: 0.9418 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1225 - precision: 0.9647 - recall: 0.9571 - accuracy: 0.9603\n",
      "Epoch 11: val_recall improved from 0.93955 to 0.94409, saving model to best_model_classifier_1_6classes.hdf5\n",
      "275/275 [==============================] - 1872s 7s/step - loss: 0.1225 - precision: 0.9647 - recall: 0.9571 - accuracy: 0.9603 - val_loss: 0.9232 - val_precision: 0.9497 - val_recall: 0.9441 - val_accuracy: 0.9464 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1162 - precision: 0.9677 - recall: 0.9586 - accuracy: 0.9633\n",
      "Epoch 12: val_recall did not improve from 0.94409\n",
      "275/275 [==============================] - 1864s 7s/step - loss: 0.1162 - precision: 0.9677 - recall: 0.9586 - accuracy: 0.9633 - val_loss: 3.4497 - val_precision: 0.9446 - val_recall: 0.9377 - val_accuracy: 0.9405 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1053 - precision: 0.9711 - recall: 0.9654 - accuracy: 0.9687\n",
      "Epoch 13: val_recall improved from 0.94409 to 0.94727, saving model to best_model_classifier_1_6classes.hdf5\n",
      "275/275 [==============================] - 1912s 7s/step - loss: 0.1053 - precision: 0.9711 - recall: 0.9654 - accuracy: 0.9687 - val_loss: 0.2102 - val_precision: 0.9516 - val_recall: 0.9473 - val_accuracy: 0.9482 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1005 - precision: 0.9714 - recall: 0.9658 - accuracy: 0.9690 \n",
      "Epoch 14: val_recall did not improve from 0.94727\n",
      "275/275 [==============================] - 3794s 14s/step - loss: 0.1005 - precision: 0.9714 - recall: 0.9658 - accuracy: 0.9690 - val_loss: 0.4882 - val_precision: 0.9475 - val_recall: 0.9427 - val_accuracy: 0.9450 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1099 - precision: 0.9711 - recall: 0.9640 - accuracy: 0.9670\n",
      "Epoch 15: val_recall did not improve from 0.94727\n",
      "275/275 [==============================] - 1815s 7s/step - loss: 0.1099 - precision: 0.9711 - recall: 0.9640 - accuracy: 0.9670 - val_loss: 2.0248 - val_precision: 0.9301 - val_recall: 0.9200 - val_accuracy: 0.9227 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1059 - precision: 0.9709 - recall: 0.9663 - accuracy: 0.9687\n",
      "Epoch 16: val_recall did not improve from 0.94727\n",
      "275/275 [==============================] - 1806s 7s/step - loss: 0.1059 - precision: 0.9709 - recall: 0.9663 - accuracy: 0.9687 - val_loss: 0.4437 - val_precision: 0.9405 - val_recall: 0.9341 - val_accuracy: 0.9377 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1039 - precision: 0.9724 - recall: 0.9659 - accuracy: 0.9695\n",
      "Epoch 17: val_recall did not improve from 0.94727\n",
      "275/275 [==============================] - 1878s 7s/step - loss: 0.1039 - precision: 0.9724 - recall: 0.9659 - accuracy: 0.9695 - val_loss: 0.2593 - val_precision: 0.9447 - val_recall: 0.9395 - val_accuracy: 0.9409 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0986 - precision: 0.9731 - recall: 0.9676 - accuracy: 0.9717\n",
      "Epoch 18: val_recall improved from 0.94727 to 0.95227, saving model to best_model_classifier_1_6classes.hdf5\n",
      "275/275 [==============================] - 1810s 7s/step - loss: 0.0986 - precision: 0.9731 - recall: 0.9676 - accuracy: 0.9717 - val_loss: 0.3053 - val_precision: 0.9606 - val_recall: 0.9523 - val_accuracy: 0.9550 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0871 - precision: 0.9778 - recall: 0.9736 - accuracy: 0.9756 \n",
      "Epoch 19: val_recall did not improve from 0.95227\n",
      "275/275 [==============================] - 5474s 20s/step - loss: 0.0871 - precision: 0.9778 - recall: 0.9736 - accuracy: 0.9756 - val_loss: 1.2015 - val_precision: 0.9561 - val_recall: 0.9500 - val_accuracy: 0.9536 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0839 - precision: 0.9769 - recall: 0.9726 - accuracy: 0.9743\n",
      "Epoch 20: val_recall did not improve from 0.95227\n",
      "275/275 [==============================] - 1851s 7s/step - loss: 0.0839 - precision: 0.9769 - recall: 0.9726 - accuracy: 0.9743 - val_loss: 0.3146 - val_precision: 0.9397 - val_recall: 0.9355 - val_accuracy: 0.9368 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0987 - precision: 0.9724 - recall: 0.9679 - accuracy: 0.9695\n",
      "Epoch 21: val_recall did not improve from 0.95227\n",
      "275/275 [==============================] - 1835s 7s/step - loss: 0.0987 - precision: 0.9724 - recall: 0.9679 - accuracy: 0.9695 - val_loss: 0.3171 - val_precision: 0.9388 - val_recall: 0.9341 - val_accuracy: 0.9364 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.1118 - precision: 0.9676 - recall: 0.9621 - accuracy: 0.9650\n",
      "Epoch 22: val_recall did not improve from 0.95227\n",
      "275/275 [==============================] - 12001s 44s/step - loss: 0.1118 - precision: 0.9676 - recall: 0.9621 - accuracy: 0.9650 - val_loss: 9.6899 - val_precision: 0.9416 - val_recall: 0.9386 - val_accuracy: 0.9400 - lr: 1.0000e-04\n",
      "Epoch 23/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0922 - precision: 0.9767 - recall: 0.9726 - accuracy: 0.9740 \n",
      "Epoch 23: val_recall improved from 0.95227 to 0.95727, saving model to best_model_classifier_1_6classes.hdf5\n",
      "275/275 [==============================] - 3150s 11s/step - loss: 0.0922 - precision: 0.9767 - recall: 0.9726 - accuracy: 0.9740 - val_loss: 0.1805 - val_precision: 0.9603 - val_recall: 0.9573 - val_accuracy: 0.9586 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0743 - precision: 0.9797 - recall: 0.9761 - accuracy: 0.9781 \n",
      "Epoch 24: val_recall did not improve from 0.95727\n",
      "275/275 [==============================] - 3261s 12s/step - loss: 0.0743 - precision: 0.9797 - recall: 0.9761 - accuracy: 0.9781 - val_loss: 0.3188 - val_precision: 0.9419 - val_recall: 0.9359 - val_accuracy: 0.9391 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0833 - precision: 0.9784 - recall: 0.9748 - accuracy: 0.9760\n",
      "Epoch 25: val_recall did not improve from 0.95727\n",
      "275/275 [==============================] - 2273s 8s/step - loss: 0.0833 - precision: 0.9784 - recall: 0.9748 - accuracy: 0.9760 - val_loss: 0.1874 - val_precision: 0.9560 - val_recall: 0.9482 - val_accuracy: 0.9518 - lr: 1.0000e-04\n",
      "Epoch 26/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0753 - precision: 0.9796 - recall: 0.9765 - accuracy: 0.9782\n",
      "Epoch 26: val_recall did not improve from 0.95727\n",
      "275/275 [==============================] - 2467s 9s/step - loss: 0.0753 - precision: 0.9796 - recall: 0.9765 - accuracy: 0.9782 - val_loss: 0.2120 - val_precision: 0.9503 - val_recall: 0.9473 - val_accuracy: 0.9473 - lr: 1.0000e-04\n",
      "Epoch 27/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0674 - precision: 0.9843 - recall: 0.9820 - accuracy: 0.9832\n",
      "Epoch 27: val_recall improved from 0.95727 to 0.95909, saving model to best_model_classifier_1_6classes.hdf5\n",
      "275/275 [==============================] - 2420s 9s/step - loss: 0.0674 - precision: 0.9843 - recall: 0.9820 - accuracy: 0.9832 - val_loss: 0.1843 - val_precision: 0.9622 - val_recall: 0.9591 - val_accuracy: 0.9591 - lr: 1.0000e-04\n",
      "Epoch 28/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0631 - precision: 0.9837 - recall: 0.9810 - accuracy: 0.9827\n",
      "Epoch 28: val_recall did not improve from 0.95909\n",
      "275/275 [==============================] - 2328s 8s/step - loss: 0.0631 - precision: 0.9837 - recall: 0.9810 - accuracy: 0.9827 - val_loss: 0.2265 - val_precision: 0.9475 - val_recall: 0.9427 - val_accuracy: 0.9445 - lr: 1.0000e-04\n",
      "Epoch 29/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0744 - precision: 0.9800 - recall: 0.9771 - accuracy: 0.9784 \n",
      "Epoch 29: val_recall did not improve from 0.95909\n",
      "275/275 [==============================] - 5796s 21s/step - loss: 0.0744 - precision: 0.9800 - recall: 0.9771 - accuracy: 0.9784 - val_loss: 0.2574 - val_precision: 0.9562 - val_recall: 0.9523 - val_accuracy: 0.9541 - lr: 1.0000e-04\n",
      "Epoch 30/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0909 - precision: 0.9760 - recall: 0.9709 - accuracy: 0.9734\n",
      "Epoch 30: val_recall did not improve from 0.95909\n",
      "275/275 [==============================] - 1873s 7s/step - loss: 0.0909 - precision: 0.9760 - recall: 0.9709 - accuracy: 0.9734 - val_loss: 4.5454 - val_precision: 0.9384 - val_recall: 0.9350 - val_accuracy: 0.9368 - lr: 1.0000e-04\n",
      "Epoch 31/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0639 - precision: 0.9837 - recall: 0.9814 - accuracy: 0.9827\n",
      "Epoch 31: val_recall did not improve from 0.95909\n",
      "275/275 [==============================] - 1882s 7s/step - loss: 0.0639 - precision: 0.9837 - recall: 0.9814 - accuracy: 0.9827 - val_loss: 0.1854 - val_precision: 0.9621 - val_recall: 0.9577 - val_accuracy: 0.9595 - lr: 1.0000e-04\n",
      "Epoch 32/50\n",
      "275/275 [==============================] - ETA: 0s - loss: 0.0594 - precision: 0.9850 - recall: 0.9826 - accuracy: 0.9835\n",
      "Epoch 32: val_recall did not improve from 0.95909\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "275/275 [==============================] - 1880s 7s/step - loss: 0.0594 - precision: 0.9850 - recall: 0.9826 - accuracy: 0.9835 - val_loss: 0.1967 - val_precision: 0.9517 - val_recall: 0.9486 - val_accuracy: 0.9500 - lr: 1.0000e-04\n",
      "Epoch 33/50\n",
      " 94/275 [=========>....................] - ETA: 19:05 - loss: 0.0535 - precision: 0.9867 - recall: 0.9860 - accuracy: 0.9864"
     ]
    }
   ],
   "source": [
    "# We will use flow() since we already have arrays\n",
    "# ----------------------------\n",
    "# Training settings\n",
    "# ----------------------------\n",
    "num_epochs = 50  # as requested\n",
    "l2_lambda = 0.001\n",
    "dropout_rate = 0.5\n",
    "classifier_choice = 1  # choose model: 1-ResNet152, 2-ResNet50, 3-EfficientNetB7, 4-MobileNet, 5-InceptionV3\n",
    "\n",
    "# Set batch size depending on model family\n",
    "if classifier_choice in (1, 2, 3):  # ResNets and EfficientNetB7\n",
    "    batch_size = 32\n",
    "else:\n",
    "    batch_size = 64\n",
    "\n",
    "# Build model\n",
    "classifier = buildClassifier(num_classes, classifier_choice, l2_lambda=l2_lambda, dropout_rate=dropout_rate)\n",
    "\n",
    "# Callbacks: checkpoint (monitor recall on validation), reduce lr, early stopping\n",
    "checkpoint_path = f'best_model_classifier_{classifier_choice}_6classes.hdf5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_recall', verbose=1,\n",
    "                             save_best_only=True, mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_recall', factor=0.5, patience=5, verbose=1, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_recall', patience=10, verbose=1, restore_best_weights=True, mode='max')\n",
    "\n",
    "# Fit using data generator\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "history = classifier.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = math.ceil(len(X_train) / batch_size),\n",
    "    epochs = num_epochs,\n",
    "    validation_data = (X_validation, y_validation),\n",
    "    callbacks = [checkpoint, reduce_lr, early_stop],\n",
    "    verbose = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Evaluation on test set\n",
    "# ----------------------------\n",
    "print(\"\\n ----- Results ------\")\n",
    "# load best weights (ModelCheckpoint already saved best)\n",
    "classifier.load_weights(checkpoint_path)\n",
    "y_pred_proba = classifier.predict(X_test, batch_size=64)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# multilabel confusion matrix (per class)\n",
    "mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "metrics = printResults(mcm)\n",
    "\n",
    "# Compute accuracy count for Wilson CI\n",
    "acc_count = np.sum(y_true == y_pred)\n",
    "acc_n = len(y_true)\n",
    "acc = acc_count / acc_n\n",
    "ci_lower, ci_upper = wilson_ci(acc_count, acc_n)\n",
    "\n",
    "print(f\"\\nOverall accuracy: {acc:.4f} (95% CI: {ci_lower:.4f} - {ci_upper:.4f})\")\n",
    "\n",
    "# ----------------------------\n",
    "# Per-class ROC AUC (one-vs-rest)\n",
    "# ----------------------------\n",
    "# Binarize true labels for multiclass AUC\n",
    "y_test_binarized = label_binarize(y_true, classes=list(range(num_classes)))\n",
    "per_class_auc = {}\n",
    "for i in range(num_classes):\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "    except ValueError:\n",
    "        auc = float('nan')  # if one class missing in test set\n",
    "    per_class_auc[i] = auc\n",
    "    print(f\"Class {i} AUC: {auc:.3f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Binary grouping ROC AUC for Normal/LSIL vs HSIL/CA\n",
    "# ----------------------------\n",
    "# Assumed class index mapping (based on your manuscript/table):\n",
    "# 0: ASC-H, 1: ASC-US, 2: CA, 3: HSIL, 4: LSIL, 5: Normal\n",
    "low_grade_indices = [5, 4]   # Normal, LSIL\n",
    "high_grade_indices = [3, 2]  # HSIL, CA\n",
    "\n",
    "# Select samples that are in either low_grade or high_grade\n",
    "mask = np.isin(y_true, low_grade_indices + high_grade_indices)\n",
    "y_true_bin = y_true[mask]\n",
    "y_scores_bin = y_pred_proba[mask]\n",
    "\n",
    "# Create binary labels: 0 = low-grade (Normal/LSIL), 1 = high-grade (HSIL/CA)\n",
    "y_true_binary = np.array([1 if t in high_grade_indices else 0 for t in y_true_bin])\n",
    "# Choose score as max probability among high-grade classes vs low-grade classes\n",
    "# We'll take prob_high = sum(probs for high_grade_indices)\n",
    "prob_high = np.sum(y_scores_bin[:, high_grade_indices], axis=1)\n",
    "try:\n",
    "    binary_auc = roc_auc_score(y_true_binary, prob_high)\n",
    "    print(f\"\\nBinary grouping (Normal/LSIL vs HSIL/CA) AUC: {binary_auc:.3f}\")\n",
    "except ValueError:\n",
    "    print(\"\\nBinary grouping AUC could not be computed (not enough class examples).\")\n",
    "\n",
    "# ----------------------------\n",
    "# Save some artifacts if desired\n",
    "# ----------------------------\n",
    "# np.save('y_test.npy', y_true)\n",
    "# np.save('y_pred.npy', y_pred)\n",
    "# np.save('y_pred_proba.npy', y_pred_proba)\n",
    "\n",
    "# ----------------------------\n",
    "# Notes printed for reproducibility summary\n",
    "# ----------------------------\n",
    "print(\"\\nTraining summary:\")\n",
    "print(f\"- Model (classifier_choice) = {classifier_choice}\")\n",
    "print(f\"- Pretrained ImageNet weights used and model fine-tuned\")\n",
    "print(f\"- Optimizer = Adam(lr=1e-4, beta1=0.9, beta2=0.999)\")\n",
    "print(f\"- Loss = categorical_crossentropy\")\n",
    "print(f\"- Epochs (max) = {num_epochs}, EarlyStopping patience = 10 (on val_recall)\")\n",
    "print(f\"- Batch size = {batch_size}\")\n",
    "print(f\"- Dropout rate (FC head) = {dropout_rate}, L2 lambda = {l2_lambda}\")\n",
    "print(\"- Data augmentation applied (rotation, shift, shear, zoom, horizontal flip)\")\n",
    "print(\"- Metrics: precision, recall, F1 (from confusion matrices), accuracy + 95% Wilson CI, per-class ROC AUC, binary ROC AUC for Normal/LSIL vs HSIL/CA\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
